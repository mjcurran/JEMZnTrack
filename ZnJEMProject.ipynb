{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79fc5d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import wideresnet\n",
    "import pdb\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from zntrack import ZnTrackProject, Node, config, dvc, zn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from zntrack.metadata import TimeIt\n",
    "#from jemsharedclasses import Base, JEMUtils\n",
    "\n",
    "\n",
    "config.nb_name = \"ZnJEMProject.ipynb\"\n",
    "project = ZnTrackProject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4879f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:18:34,507 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n",
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup parameters\n",
    "# defaults for paper\n",
    "# --lr .0001 --dataset cifar10 --optimizer adam --p_x_weight 1.0 --p_y_given_x_weight 1.0 \n",
    "# --p_x_y_weight 0.0 --sigma .03 --width 10 --depth 28 --save_dir /YOUR/SAVE/DIR \n",
    "# --plot_uncond --warmup_iters 1000\n",
    "#\n",
    "\n",
    "#This class can be reused by passing a name attribute to its constructor, and then referencing\n",
    "# that name in the @Node class dependencies\n",
    "@Node()\n",
    "class train_args():\n",
    "    # define params\n",
    "    # this will write them to params.yaml\n",
    "    experiment = dvc.params()\n",
    "    dataset = dvc.params()\n",
    "    n_classes = dvc.params()\n",
    "    n_steps = dvc.params()\n",
    "    width = dvc.params()\n",
    "    depth = dvc.params()\n",
    "    sigma = dvc.params()\n",
    "    data_root = dvc.params()\n",
    "    seed = dvc.params()\n",
    "    lr = dvc.params()\n",
    "    clf_only = dvc.params()\n",
    "    labels_per_class = dvc.params()\n",
    "    batch_size = dvc.params()\n",
    "    n_epochs = dvc.params()\n",
    "    dropout_rate = dvc.params()\n",
    "    weight_decay = dvc.params()\n",
    "    norm = dvc.params()\n",
    "    save_dir = dvc.params()\n",
    "    ckpt_every = dvc.params()\n",
    "    eval_every = dvc.params()\n",
    "    print_every = dvc.params()\n",
    "    load_path = dvc.params()\n",
    "    print_to_log = dvc.params()\n",
    "    n_valid = dvc.params()\n",
    "    \n",
    "    result = zn.metrics()\n",
    "    \n",
    "    def __call__(self, param_dict):\n",
    "        # set defaults\n",
    "        self.experiment = \"energy_model\"\n",
    "        self.dataset = \"cifar10\"\n",
    "        self.n_classes = 10\n",
    "        self.n_steps = 20\n",
    "        self.width = 10 # wide-resnet widen_factor\n",
    "        self.depth = 28  # wide-resnet depth\n",
    "        self.sigma = .03 # image transformation\n",
    "        self.data_root = \"./dataset\" \n",
    "        self.seed = JEMUtils.get_parameter(\"seed\", 1)\n",
    "        # optimization\n",
    "        self.lr = 1e-4\n",
    "        self.clf_only = False #action=\"store_true\", help=\"If set, then only train the classifier\")\n",
    "        self.labels_per_class = -1# help=\"number of labeled examples per class, if zero then use all labels\")\n",
    "        self.batch_size = 64\n",
    "        self.n_epochs = JEMUtils.get_parameter(\"epochs\", 10)\n",
    "        # regularization\n",
    "        self.dropout_rate = 0.0\n",
    "        self.sigma = 3e-2 # help=\"stddev of gaussian noise to add to input, .03 works but .1 is more stable\")\n",
    "        self.weight_decay = 0.0\n",
    "        # network\n",
    "        self.norm = None # choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"], help=\"norm to add to weights, none works fine\")\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.ckpt_every = 1 # help=\"Epochs between checkpoint save\")\n",
    "        self.eval_every = 1 # help=\"Epochs between evaluation\")\n",
    "        self.print_every = 100 # help=\"Iterations between print\")\n",
    "        self.load_path = None # path for checkpoint to load\n",
    "        self.print_to_log = False #\", action=\"store_true\", help=\"If true, directs std-out to log file\")\n",
    "        self.n_valid = 5000 # number of validation images\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])\n",
    "            \n",
    "    def run(self):\n",
    "        self.result = self.experiment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3107da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a base for the Node compute functions, to split off the actual work from the dvc control flow\n",
    "class Base:\n",
    "    def compute(self, inp):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3c7c27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random subset of data\n",
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bb4fa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Wide_ResNet\n",
    "# Uses The Google Research Authors, file wideresnet.py\n",
    "class FTrain(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(FTrain, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm, dropout_rate=dropout_rate)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, n_classes)\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1511748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JEMUtils:\n",
    "    \n",
    "    \n",
    "    # various utilities\n",
    "    @staticmethod\n",
    "    def cycle(loader):\n",
    "        while True:\n",
    "            for data in loader:\n",
    "                yield data\n",
    "                \n",
    "                \n",
    "    # calculate loss and accuracy for periodic printout\n",
    "    \n",
    "    @staticmethod\n",
    "    def eval_classification(f, dload, device):\n",
    "        corrects, losses = [], []\n",
    "        for x_p_d, y_p_d in dload:\n",
    "            x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "            logits = f.classify(x_p_d)\n",
    "            loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().numpy()\n",
    "            losses.extend(loss)\n",
    "            correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "            corrects.extend(correct)\n",
    "        loss = np.mean(losses)\n",
    "        correct = np.mean(corrects)\n",
    "        return correct, loss\n",
    "    \n",
    "    \n",
    "    # save checkpoint data\n",
    "    \n",
    "    @staticmethod\n",
    "    def checkpoint(f, opt, epoch_no, tag, args, device):\n",
    "        f.cpu()\n",
    "        ckpt_dict = {\n",
    "            \"model_state_dict\": f.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'epoch': epoch_no,\n",
    "            #\"replay_buffer\": buffer\n",
    "        }\n",
    "        t.save(ckpt_dict, os.path.join(os.path.join(args.save_dir, args.experiment), tag))\n",
    "        f.to(device)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def get_parameter(name, default):\n",
    "        params = yaml.safe_load(open(\"params.yaml\"))\n",
    "        to_search = params\n",
    "        for part in name.split(\".\"):\n",
    "            result = to_search.get(part)\n",
    "            if result == None:\n",
    "                return default\n",
    "            to_search = result\n",
    "        return to_search\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_data(args):\n",
    "        im_sz = 32\n",
    "        \n",
    "        \n",
    "        #global transform_train\n",
    "        # the GaussianBlur is roughly equivalent to the lambda functions here\n",
    "        # but the lambda functions aren't serializable for multi-processing\n",
    "        # torchvision.transforms documentation state to not use lambda functions as well\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.RandomHorizontalFlip(),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             tr.GaussianBlur(kernel_size=(5, 5), sigma=(args.sigma, args.sigma * 2))]\n",
    "             #lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "        #global transform_test\n",
    "        transform_test = tr.Compose(\n",
    "            [tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             tr.GaussianBlur(kernel_size=(5, 5), sigma=(args.sigma, args.sigma * 2))]\n",
    "             #lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        def dataset_fn(train, transform):\n",
    "            return tv.datasets.CIFAR10(root='./dataset', transform=transform, download=True, train=train)\n",
    "\n",
    "        # get all training inds\n",
    "        #global full_train\n",
    "        full_train = dataset_fn(train=True, transform=transform_train)\n",
    "        all_inds = list(range(len(full_train)))\n",
    "        # set seed\n",
    "        np.random.seed(args.seed)\n",
    "        # shuffle\n",
    "        np.random.shuffle(all_inds)\n",
    "        # seperate out validation set\n",
    "        if args.n_valid is not None:\n",
    "            valid_inds, train_inds = all_inds[:args.n_valid], all_inds[args.n_valid:]\n",
    "        else:\n",
    "            valid_inds, train_inds = [], all_inds\n",
    "        train_inds = np.array(train_inds)\n",
    "        train_labeled_inds = []\n",
    "        other_inds = []\n",
    "        train_labels = np.array([full_train[ind][1] for ind in train_inds])\n",
    "        if args.labels_per_class > 0:\n",
    "            for i in range(args.n_classes):\n",
    "                print(i)\n",
    "                train_labeled_inds.extend(train_inds[train_labels == i][:args.labels_per_class])\n",
    "                other_inds.extend(train_inds[train_labels == i][args.labels_per_class:])\n",
    "        else:\n",
    "            train_labeled_inds = train_inds\n",
    "\n",
    "        dset_train = DataSubset(\n",
    "            dataset_fn(train=True, transform=transform_train),\n",
    "            inds=train_inds)\n",
    "        dset_train_labeled = DataSubset(\n",
    "            dataset_fn(train=True, transform=transform_train),\n",
    "            inds=train_labeled_inds)\n",
    "        dset_valid = DataSubset(\n",
    "            dataset_fn(train=True, transform=transform_test),\n",
    "            inds=valid_inds)\n",
    "        \n",
    "        dload_train = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "        dload_train_labeled = DataLoader(dset_train_labeled, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "        dload_train_labeled = JEMUtils.cycle(dload_train_labeled)\n",
    "        dset_test = dataset_fn(train=False, transform=transform_test)\n",
    "        dload_valid = DataLoader(dset_valid, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "        dload_test = DataLoader(dset_test, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "        return dload_train, dload_train_labeled, dload_valid, dload_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2aef4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic training from train.ipynb\n",
    "class Trainer(Base):\n",
    "    \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        \n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = FTrain(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path and os.path.exists(os.path.join(os.path.join(args.load_path, args.experiment), 'ckpt_9.pt')):\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            #ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), 'ckpt_9.pt'))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "    \n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # normal cross entropy loss function\n",
    "                # maximize log p(y | x)\n",
    "                logits = f.classify(x_lab)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergwence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "\n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            JEMUtils.checkpoint(f, optim, epoch, \"last_ckpt.pt\", args, device)\n",
    "\n",
    "        # write stats\n",
    "        with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "            json.dump(scores, outfile)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "766ee5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:18:49,396 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n",
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n"
     ]
    }
   ],
   "source": [
    "#Do the operations from train.ipynb and track in dvc\n",
    "#dependency is train_args stage with default name\n",
    "#outs is the path to the last_ckpt.pt model file, which serves as a dependency to the evaluation stage\n",
    "\n",
    "@Node()\n",
    "class XEntropyAugmented:\n",
    "    \n",
    "    args: train_args = dvc.deps(train_args(load=True))\n",
    "    trainer: Base = zn.Method()\n",
    "    result = zn.metrics()\n",
    "    model: Path = dvc.outs()  # is making the model file an outs causing it to delete the file?\n",
    "    \n",
    "            \n",
    "    def __call__(self, operation):\n",
    "        self.trainer = operation\n",
    "        self.model = Path(os.path.join(os.path.join(self.args.save_dir, self.args.experiment), \"last_ckpt.pt\"))\n",
    "    \n",
    "    @TimeIt\n",
    "    def run(self):\n",
    "        #self.result = self.args.result\n",
    "        self.result = self.trainer.compute(self.args)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d597729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:18:56,713 (WARNING): --- Writing new DVC file! ---\n",
      "2022-01-10 13:18:57,782 (INFO): Modifying stage 'train_args' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n",
      "2022-01-10 13:18:57,793 (ERROR): Can not convert args!\n",
      "2022-01-10 13:18:57,794 (ERROR): Can not convert kwargs!\n",
      "2022-01-10 13:18:57,831 (WARNING): --- Writing new DVC file! ---\n",
      "2022-01-10 13:18:58,663 (INFO): Modifying stage 'XEntropyAugmented' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add/change parameters for this stage\n",
    "inline_parms = {\"lr\": .0001, \"experiment\": 'x-entropy_augmented', \"load_path\": './experiment'}\n",
    "\n",
    "#declare the train_args stage and pass the modified/new params\n",
    "params = train_args()\n",
    "params(param_dict=inline_parms)\n",
    "\n",
    "#declare the compute class for the XEntropyAugmented stage\n",
    "trainer = Trainer()\n",
    "\n",
    "#declare stage and pass the compute class\n",
    "runner = XEntropyAugmented()\n",
    "runner(operation=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3140aa6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:03,588 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n",
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n"
     ]
    }
   ],
   "source": [
    "#stage MaxEntropyL1, originally from train_max_entropy_L1.ipynb\n",
    "# dependency is train_args, named \"train_argsL1\"\n",
    "#outs is the path to the last_ckpt.pt model file, which serves as a dependency to the evaluation stage\n",
    "\n",
    "@Node()\n",
    "class MaxEntropyL1:\n",
    "    args: train_args = dvc.deps(train_args(load=True, name=\"train_argsL1\"))\n",
    "    trainer: Base = zn.Method()\n",
    "    result = zn.metrics()\n",
    "    model: Path = dvc.outs()\n",
    "            \n",
    "    def __call__(self, operation):\n",
    "        self.trainer = operation\n",
    "        self.model = Path(os.path.join(os.path.join(self.args.save_dir, self.args.experiment), \"last_ckpt.pt\"))\n",
    "    \n",
    "    @TimeIt\n",
    "    def run(self):\n",
    "        #self.result = self.args.result\n",
    "        self.result = self.trainer.compute(self.args)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093ae3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer class for MaxEntropyL1 stage's compute function\n",
    "\n",
    "class TrainerL1(Base):\n",
    "    \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "        \n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = FTrain(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path and os.path.exists(os.path.join(os.path.join(args.load_path, args.experiment), 'ckpt_9.pt')):\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            #ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), 'ckpt_9.pt'))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # get logits for calculations\n",
    "                logits = f.classify(x_lab)\n",
    "\n",
    "                ####################################################\n",
    "                # Maximize entropy by assuming equal probabilities #\n",
    "                ####################################################\n",
    "                energy = logits.logsumexp(dim=1, keepdim=False)\n",
    "            \n",
    "                e_mean = t.mean(energy)\n",
    "                #print('Energy shape',energy.size())\n",
    "            \n",
    "                energy_loss = t.sum(t.abs(e_mean - energy))\n",
    "            \n",
    "                L += energy_loss\n",
    "            \n",
    "                ######################################\n",
    "                # normal cross entropy loss function #\n",
    "                ######################################\n",
    "                # maximize log p(y | x)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergwence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "            \n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            JEMUtils.checkpoint(f, optim, epoch, \"last_ckpt.pt\", args, device)\n",
    "\n",
    "        # write stats\n",
    "        with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "            json.dump(scores, outfile)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64d0ec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:08,891 (WARNING): --- Writing new DVC file! ---\n",
      "2022-01-10 13:19:09,742 (INFO): Modifying stage 'train_argsL1' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n",
      "2022-01-10 13:19:09,754 (ERROR): Can not convert args!\n",
      "2022-01-10 13:19:09,755 (ERROR): Can not convert kwargs!\n",
      "2022-01-10 13:19:09,800 (WARNING): --- Writing new DVC file! ---\n",
      "2022-01-10 13:19:10,572 (INFO): Modifying stage 'MaxEntropyL1' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inline_parms1 = {\"lr\": .0001, \"experiment\": 'max-entropy-L1_augmented', \"load_path\": './experiment'} \n",
    "\n",
    "#declare the train_args with name train_argsL1, which becomes its stage name in dvc.yaml\n",
    "params1 = train_args(name=\"train_argsL1\")\n",
    "params1(param_dict=inline_parms1)\n",
    "\n",
    "trainer1 = TrainerL1()\n",
    "\n",
    "runner1 = MaxEntropyL1()\n",
    "runner1(operation=trainer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36910cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:13,729 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n",
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n"
     ]
    }
   ],
   "source": [
    "#stage MaxEntropyL2 originally from train_max_entropy_L2.ipynb\n",
    "#dependency is train_args with name train_argsL2\n",
    "\n",
    "@Node()\n",
    "class MaxEntropyL2:\n",
    "    args: train_args = dvc.deps(train_args(load=True, name=\"train_argsL2\"))\n",
    "    trainer: Base = zn.Method()\n",
    "    result = zn.metrics()\n",
    "    model: Path = dvc.outs()\n",
    "            \n",
    "    def __call__(self, operation):\n",
    "        self.trainer = operation\n",
    "        self.model = Path(os.path.join(os.path.join(self.args.save_dir, self.args.experiment), \"last_ckpt.pt\"))\n",
    "    \n",
    "    @TimeIt\n",
    "    def run(self):\n",
    "        #self.result = self.args.result\n",
    "        self.result = self.trainer.compute(self.args)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a92fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute class for the above stage\n",
    "\n",
    "class TrainerL2(Base):\n",
    "    \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "        \n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = FTrain(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path and os.path.exists(os.path.join(os.path.join(args.load_path, args.experiment), 'ckpt_9.pt')):\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            #ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), 'ckpt_9.pt'))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "    \n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # get logits for calculations\n",
    "                logits = f.classify(x_lab)\n",
    "\n",
    "                ####################################################\n",
    "                # Maximize entropy by assuming equal probabilities #\n",
    "                ####################################################\n",
    "                energy = logits.logsumexp(dim=1, keepdim=False)\n",
    "            \n",
    "                e_mean = t.mean(energy)\n",
    "                #print('Energy shape',energy.size())\n",
    "            \n",
    "                energy_loss = t.sum((e_mean - energy)**2)\n",
    "            \n",
    "                L += energy_loss\n",
    "            \n",
    "                ######################################\n",
    "                # normal cross entropy loss function #\n",
    "                ######################################\n",
    "                # maximize log p(y | x)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergwence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "\n",
    "            \n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            JEMUtils.checkpoint(f, optim, epoch, \"last_ckpt.pt\", args, device)\n",
    "\n",
    "        # write stats\n",
    "        with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "            json.dump(scores, outfile)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69807269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:22,412 (WARNING): --- Writing new DVC file! ---\n",
      "2022-01-10 13:19:23,256 (INFO): Modifying stage 'train_argsL2' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n",
      "2022-01-10 13:19:23,267 (ERROR): Can not convert args!\n",
      "2022-01-10 13:19:23,268 (ERROR): Can not convert kwargs!\n",
      "2022-01-10 13:19:23,306 (WARNING): --- Writing new DVC file! ---\n",
      "2022-01-10 13:19:24,088 (INFO): Modifying stage 'MaxEntropyL2' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inline_parms2 = {\"lr\": .0001, \"experiment\": 'max-entropy-L2_augmented', \"load_path\": './experiment'} \n",
    "\n",
    "#declare the train_args with new name\n",
    "params2 = train_args(name=\"train_argsL2\")\n",
    "params2(param_dict=inline_parms2)\n",
    "trainer2 = TrainerL2()\n",
    "\n",
    "runner2 = MaxEntropyL2()\n",
    "runner2(operation=trainer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c2e356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, 10)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4a27fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            return t.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a432ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:29,849 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n",
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n"
     ]
    }
   ],
   "source": [
    "#class to hold the parameters for the evaluate calibration stage\n",
    "\n",
    "@Node()\n",
    "\n",
    "# Setup parameters\n",
    "class eval_args():\n",
    "    \n",
    "    experiment = dvc.params()\n",
    "    dataset = dvc.params()\n",
    "    n_steps = dvc.params()\n",
    "    width = dvc.params()\n",
    "    depth = dvc.params()\n",
    "    sigma = dvc.params()\n",
    "    data_root = dvc.params()\n",
    "    seed = dvc.params()    \n",
    "    norm = dvc.params()\n",
    "    save_dir = dvc.params()\n",
    "    print_to_log = dvc.params()\n",
    "    uncond = dvc.params()\n",
    "    load_path = dvc.params()\n",
    "    \n",
    "    result = zn.metrics()\n",
    "    \n",
    "    #src = dvc.deps(Path(\"src\", self.experiment))\n",
    "\n",
    "    \n",
    "    def __call__(self, param_dict):\n",
    "        self.experiment = \"energy_model\"\n",
    "        self.data_root = \"./dataset\" \n",
    "        self.dataset = \"cifar_test\" #, type=str, choices=[\"cifar_train\", \"cifar_test\", \"svhn_test\", \"svhn_train\"], help=\"Dataset to use when running test_clf for classification accuracy\")\n",
    "        self.seed = JEMUtils.get_parameter(\"seed\", 1)\n",
    "        # regularization\n",
    "        self.sigma = 3e-2\n",
    "        # network\n",
    "        self.norm = None #, choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"])\n",
    "        # EBM specific\n",
    "        self.n_steps = 20 # help=\"number of steps of SGLD per iteration, 100 works for short-run, 20 works for PCD\")\n",
    "        self.width = 10 # help=\"WRN width parameter\")\n",
    "        self.depth = 28 # help=\"WRN depth parameter\")        \n",
    "        self.uncond = False # \"store_true\" # help=\"If set, then the EBM is unconditional\")\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.print_to_log = False\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])\n",
    "            \n",
    "    def run(self):\n",
    "        self.result = {\"experiment\": self.experiment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f717b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class for the evaluation stage\n",
    "\n",
    "class Calibration(Base):\n",
    "    \n",
    "    def calibration(self, f, args, device):\n",
    "        transform_test = tr.Compose(\n",
    "            [tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + t.randn_like(x) * args.sigma]\n",
    "        )\n",
    "\n",
    "        def sample(x, n_steps=args.n_steps):\n",
    "            x_k = t.autograd.Variable(x.clone(), requires_grad=True)\n",
    "            # sgld\n",
    "            for k in range(n_steps):\n",
    "                f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "                x_k.data += f_prime + 1e-2 * t.randn_like(x_k)\n",
    "            final_samples = x_k.detach()\n",
    "            return final_samples\n",
    "\n",
    "        if args.dataset == \"cifar_train\":\n",
    "            dset = tv.datasets.CIFAR10(root=args.data_root, transform=transform_test, download=True, train=True)\n",
    "        elif args.dataset == \"cifar_test\":\n",
    "            dset = tv.datasets.CIFAR10(root=args.data_root, transform=transform_test, download=True, train=False)\n",
    "        elif args.dataset == \"svhn_train\":\n",
    "            dset = tv.datasets.SVHN(root=args.data_root, transform=transform_test, download=True, split=\"train\")\n",
    "        else:  # args.dataset == \"svhn_test\":\n",
    "            dset = tv.datasets.SVHN(root=args.data_root, transform=transform_test, download=True, split=\"test\")\n",
    "\n",
    "        dload = DataLoader(dset, batch_size=1, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "        start=0.05\n",
    "        step=.05\n",
    "        num=20\n",
    "\n",
    "        bins=np.arange(0,num)*step+start+ 1e-10\n",
    "        bin_total = np.zeros(20)+1e-5\n",
    "        bin_correct = np.zeros(20)\n",
    "\n",
    "        #energies, corrects, losses, pys, preds = [], [], [], [], []\n",
    "    \n",
    "        for x_p_d, y_p_d in tqdm(dload):\n",
    "            x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "\n",
    "            logits = f.classify(x_p_d).detach().cpu()#.numpy()\n",
    "\n",
    "            py = nn.Softmax()(logits)[0].numpy()#(f.classify(x_p_d)).max(1)[0].detach().cpu().numpy()\n",
    "        \n",
    "            expected = y_p_d[0].detach().cpu().numpy()\n",
    "        \n",
    "            actual = logits.max(1)[1][0].numpy()\n",
    "        \n",
    "            #print(py[expected],expected,actual)\n",
    "        \n",
    "            inds = np.digitize(py[actual], bins)\n",
    "            bin_total[inds] += 1\n",
    "            if actual == expected:\n",
    "                bin_correct[inds] += 1\n",
    "            \n",
    "        #\n",
    "        accu = np.divide(bin_correct,bin_total)\n",
    "        print(\"Bin data\",np.sum(bin_total),accu,bins,bin_total)\n",
    "    \n",
    "        # calc ECE\n",
    "        ECE = 0.0\n",
    "        for i in range(20):\n",
    "            #print(\"accu\",accu[i],(i/20.0 + 0.025),bin_total[i])\n",
    "            ECE += (float(bin_total[i]) / float(np.sum(bin_total))) * abs(accu[i] - (i/20.0 + 0.025))\n",
    "        \n",
    "        print(\"ECE\", ECE)\n",
    "    \n",
    "        # save calibration  in a text file\n",
    "            \n",
    "        pd.DataFrame({'accuracy': accu, 'ECE': ECE}).to_csv(path_or_buf=os.path.join(args.save_dir, args.experiment) + \"_calibration.csv\", index_label=\"index\")\n",
    "        outputcsv = os.path.join(args.save_dir, args.experiment) + \"_calibration.csv\"\n",
    "        return outputcsv\n",
    "        \n",
    "        \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        model_cls = F if args.uncond else CCF\n",
    "        f = model_cls(args.depth, args.width, args.norm)\n",
    "        print(f\"loading model from {os.path.join(os.path.join(args.load_path, args.experiment), 'last_ckpt.pt')}\")\n",
    "\n",
    "        # load em up\n",
    "        ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), 'last_ckpt.pt'))\n",
    "        f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "        #replay_buffer = ckpt_dict[\"replay_buffer\"]\n",
    "\n",
    "        f = f.to(device)\n",
    "\n",
    "        # do calibration\n",
    "        resultfile = self.calibration(f, args, device)\n",
    "        return resultfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a0f8d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:35,009 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n",
      "2022-01-10 13:19:35,177 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:37,096 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n",
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:39,021 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n",
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:40,887 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n",
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:42,837 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n",
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:44,733 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n",
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n",
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n",
      "[NbConvertApp] Converting notebook ZnJEMProject.ipynb to script\n",
      "[NbConvertApp] Writing 40669 bytes to ZnJEMProject.py\n"
     ]
    }
   ],
   "source": [
    "#stage EvaluateX\n",
    "#multiple dependencies of eval_args with appropriate names for each stage they are testing\n",
    "# and the model files from the other stages so the dependency graph flows in the correct order\n",
    "\n",
    "@Node()\n",
    "class EvaluateX:\n",
    "    \n",
    "    #from the DVC docs:  \"Stage dependencies can be any file or directory\"\n",
    "    # so the eval_args stages have to output something in order to be used as deps here\n",
    "    # so we use the metrics files like:  nodes/x-entropy_augmented/metrics_no_cache.json\n",
    "    args = dvc.deps([eval_args(load=True, name=\"x-entropy_augmented\"), \n",
    "                     eval_args(load=True, name=\"max-entropy-L1_augmented\"), \n",
    "                     eval_args(load=True, name=\"max-entropy-L2_augmented\")])\n",
    "    #arg0: eval_args = dvc.deps(eval_args(name=\"x-entropy_augmented\", load=True))\n",
    "    \n",
    "    models = dvc.deps([XEntropyAugmented(load=True), MaxEntropyL1(load=True), MaxEntropyL2(load=True)])\n",
    "    \n",
    "    calibration: Base = zn.Method()\n",
    "   \n",
    "    result: Path = dvc.outs()\n",
    "    \n",
    "    # add plots to dvc tracking\n",
    "    # this would be better if the paths could be defined by the passed args, but can't see how to \n",
    "    plot0: Path = dvc.plots(\"./experiment/x-entropy_augmented_calibration.csv\")\n",
    "    plot1: Path = dvc.plots(\"./experiment/max-entropy-L1_augmented_calibration.csv\")\n",
    "    plot2: Path = dvc.plots(\"./experiment/max-entropy-L2_augmented_calibration.csv\")\n",
    "    \n",
    "    #def __init__(self):\n",
    "    #    self.result = Path('./experiment/joint_energy_models_scores.json')\n",
    "        \n",
    "            \n",
    "    def __call__(self, operation):\n",
    "        self.calibration = operation\n",
    "        \n",
    "    \n",
    "    @TimeIt\n",
    "    def run(self):\n",
    "        #scores = {}\n",
    "        for arg in self.args:\n",
    "            self.calibration.compute(arg)\n",
    "            #with open('./experiment/joint_energy_models_scores.json', 'a') as outfile:\n",
    "            #    json.dump(scores, outfile)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "748185a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:19:52,974 (WARNING): --- Writing new DVC file! ---\n",
      "2022-01-10 13:19:53,787 (INFO): Modifying stage 'x-entropy_augmented' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n",
      "2022-01-10 13:19:53,846 (WARNING): --- Writing new DVC file! ---\n",
      "2022-01-10 13:19:54,612 (INFO): Modifying stage 'max-entropy-L1_augmented' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n",
      "2022-01-10 13:19:54,668 (WARNING): --- Writing new DVC file! ---\n",
      "2022-01-10 13:19:55,443 (INFO): Modifying stage 'max-entropy-L2_augmented' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#declare all the args for evaluation stage\n",
    "\n",
    "inline_train_args = {\"load_path\": \"./experiment\", \"experiment\": \"x-entropy_augmented\"}\n",
    "args_train = eval_args(name=\"x-entropy_augmented\")\n",
    "args_train(inline_train_args)\n",
    "\n",
    "inline_L1_args = {\"load_path\": \"./experiment\", \"experiment\": \"max-entropy-L1_augmented\"}\n",
    "args_L1 = eval_args(name=\"max-entropy-L1_augmented\")\n",
    "args_L1(inline_L1_args)\n",
    "\n",
    "inline_L2_args = {\"load_path\": \"./experiment\", \"experiment\": \"max-entropy-L2_augmented\"}\n",
    "args_L2 = eval_args(name=\"max-entropy-L2_augmented\")\n",
    "args_L2(inline_L2_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1f7d742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-10 13:20:00,391 (ERROR): Can not convert args!\n",
      "2022-01-10 13:20:00,392 (ERROR): Can not convert kwargs!\n",
      "2022-01-10 13:20:00,426 (WARNING): --- Writing new DVC file! ---\n",
      "2022-01-10 13:20:00,426 (WARNING): Found outs with value None that cannot be processed - skipping it.\n",
      "2022-01-10 13:20:01,202 (INFO): Modifying stage 'EvaluateX' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#declare the calibration compute class and pass to the evaluation stage\n",
    "\n",
    "cal = Calibration()\n",
    "eva = EvaluateX()\n",
    "eva(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "34c25d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stage 'max-entropy-L1_augmented':\n",
      "> python3 -c \"from src.eval_args import eval_args; eval_args(load=True, name='max-entropy-L1_augmented').run()\" \n",
      "Updating lock file 'dvc.lock'\n",
      "\n",
      "Running stage 'train_argsL1':\n",
      "> python3 -c \"from src.train_args import train_args; train_args(load=True, name='train_argsL1').run()\" \n",
      "Updating lock file 'dvc.lock'\n",
      "\n",
      "Running stage 'MaxEntropyL1':\n",
      "> python3 -c \"from src.MaxEntropyL1 import MaxEntropyL1; MaxEntropyL1(load=True, name='MaxEntropyL1').run()\" \n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "| Wide-Resnet 28x10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crc/python/JEMZnTrack/__pypackages__/3.9/lib/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0s/p3gqsflx1jv846fkrm_2ktnr0000gn/T/ipykernel_60971/250131282.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#see cluster-script.sh for alternately enqueueing to the CRC cluster.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mproject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/python/JEMZnTrack/__pypackages__/3.9/lib/zntrack/project/zntrack_project.py\u001b[0m in \u001b[0;36mrepro\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrepro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;34m\"\"\"Run dvc repro\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dvc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"repro\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0mcheck_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \"\"\"\n\u001b[0;32m--> 368\u001b[0;31m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mcmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"args\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1915\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1874\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1875\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1876\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this will run the project in the notebook kernel\n",
    "#see cluster-script.sh for alternately enqueueing to the CRC cluster.\n",
    "\n",
    "project.repro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f7f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
