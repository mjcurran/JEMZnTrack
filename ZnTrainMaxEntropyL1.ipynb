{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58b248f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch as t, torch.nn as nn, torch.nn.functional as tnnF, torch.distributions as tdist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import wideresnet # from The Google Research Authors\n",
    "import json\n",
    "import yaml\n",
    "from zntrack import ZnTrackProject, Node, config, dvc, zn\n",
    "from jemsharedclasses import JEMUtils, DataSubset, F, Base\n",
    "\n",
    "config.nb_name = \"ZnTrainMaxEntropyL1.ipynb\"\n",
    "project = ZnTrackProject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2734df71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 13:36:49,428 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnTrainMaxEntropyL1.ipynb to script\n",
      "[NbConvertApp] Writing 9943 bytes to ZnTrainMaxEntropyL1.py\n"
     ]
    }
   ],
   "source": [
    "# Setup parameters\n",
    "# defaults for paper\n",
    "# --lr .0001 --dataset cifar10 --optimizer adam --p_x_weight 1.0 --p_y_given_x_weight 1.0 \n",
    "# --p_x_y_weight 0.0 --sigma .03 --width 10 --depth 28 --save_dir /YOUR/SAVE/DIR \n",
    "# --plot_uncond --warmup_iters 1000\n",
    "\n",
    "@Node()\n",
    "class train_argsL1():\n",
    "    # define params\n",
    "    # this will write them to params.yaml\n",
    "    experiment = dvc.params()\n",
    "    dataset = dvc.params()\n",
    "    n_classes = dvc.params()\n",
    "    n_steps = dvc.params()\n",
    "    width = dvc.params()\n",
    "    depth = dvc.params()\n",
    "    sigma = dvc.params()\n",
    "    data_root = dvc.params()\n",
    "    seed = dvc.params()\n",
    "    lr = dvc.params()\n",
    "    clf_only = dvc.params()\n",
    "    labels_per_class = dvc.params()\n",
    "    batch_size = dvc.params()\n",
    "    n_epochs = dvc.params()\n",
    "    dropout_rate = dvc.params()\n",
    "    weight_decay = dvc.params()\n",
    "    norm = dvc.params()\n",
    "    save_dir = dvc.params()\n",
    "    ckpt_every = dvc.params()\n",
    "    eval_every = dvc.params()\n",
    "    print_every = dvc.params()\n",
    "    load_path = dvc.params()\n",
    "    print_to_log = dvc.params()\n",
    "    n_valid = dvc.params()\n",
    "    \n",
    "    result = zn.metrics()\n",
    "    \n",
    "    def __call__(self, param_dict):\n",
    "        # set defaults\n",
    "        self.experiment = \"energy_model\"\n",
    "        self.dataset = \"cifar10\"\n",
    "        self.n_classes = 10\n",
    "        self.n_steps = 20\n",
    "        self.width = 10 # wide-resnet widen_factor\n",
    "        self.depth = 28  # wide-resnet depth\n",
    "        self.sigma = .03 # image transformation\n",
    "        self.data_root = \"./dataset\" \n",
    "        self.seed = JEMUtils.get_parameter(\"seed\", 1)\n",
    "        # optimization\n",
    "        self.lr = 1e-4\n",
    "        self.clf_only = False #action=\"store_true\", help=\"If set, then only train the classifier\")\n",
    "        self.labels_per_class = -1# help=\"number of labeled examples per class, if zero then use all labels\")\n",
    "        self.batch_size = 64\n",
    "        self.n_epochs = JEMUtils.get_parameter(\"epochs\", 10)\n",
    "        # regularization\n",
    "        self.dropout_rate = 0.0\n",
    "        self.sigma = 3e-2 # help=\"stddev of gaussian noise to add to input, .03 works but .1 is more stable\")\n",
    "        self.weight_decay = 0.0\n",
    "        # network\n",
    "        self.norm = None # choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"], help=\"norm to add to weights, none works fine\")\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.ckpt_every = 10 # help=\"Epochs between checkpoint save\")\n",
    "        self.eval_every = 1 # help=\"Epochs between evaluation\")\n",
    "        self.print_every = 100 # help=\"Iterations between print\")\n",
    "        self.load_path = None # path for checkpoint to load\n",
    "        self.print_to_log = False #\", action=\"store_true\", help=\"If true, directs std-out to log file\")\n",
    "        self.n_valid = 5000 # number of validation images\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])\n",
    "            \n",
    "    def run(self):\n",
    "        self.result = self.experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "321f99f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 15:22:58,110 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnTrainMaxEntropyL1.ipynb to script\n",
      "[NbConvertApp] Writing 9999 bytes to ZnTrainMaxEntropyL1.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@Node()\n",
    "class MaxEntropyL1:\n",
    "    args: train_argsL1 = dvc.deps(train_argsL1(load=True))\n",
    "    trainer: Base = zn.Method()\n",
    "    result = zn.metrics()\n",
    "            \n",
    "    def __call__(self, operation):\n",
    "        self.trainer = operation\n",
    "    \n",
    "    def run(self):\n",
    "        self.result = self.args.result\n",
    "        self.result += self.trainer.compute(self.args)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "873f1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TrainerL1(Base):\n",
    "    \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "        \n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = F(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path:\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # get logits for calculations\n",
    "                logits = f.classify(x_lab)\n",
    "\n",
    "                ####################################################\n",
    "                # Maximize entropy by assuming equal probabilities #\n",
    "                ####################################################\n",
    "                energy = logits.logsumexp(dim=1, keepdim=False)\n",
    "            \n",
    "                e_mean = t.mean(energy)\n",
    "                #print('Energy shape',energy.size())\n",
    "            \n",
    "                energy_loss = t.sum(t.abs(e_mean - energy))\n",
    "            \n",
    "                L += energy_loss\n",
    "            \n",
    "                ######################################\n",
    "                # normal cross entropy loss function #\n",
    "                ######################################\n",
    "                # maximize log p(y | x)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergwence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "            \n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            JEMUtils.checkpoint(f, optim, epoch, \"last_ckpt.pt\", args, device)\n",
    "\n",
    "        # write stats\n",
    "        with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "            json.dump(scores, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cf785615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 13:37:56,105 (WARNING): --- Writing new DVC file! ---\n",
      "2021-12-21 13:37:56,928 (INFO): Modifying stage 'train_argsL1' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inline_parms = {\"lr\": .0001, \"experiment\": 'max-entropy-L1_augmented'} #, \"load_path\": './all1/last_ckpt.pt'} \n",
    "\n",
    "params = train_argsL1()\n",
    "params(param_dict=inline_parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2ab680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 15:23:07,434 (ERROR): Can not convert args!\n",
      "2021-12-21 15:23:07,435 (ERROR): Can not convert kwargs!\n",
      "2021-12-21 15:23:07,472 (WARNING): --- Writing new DVC file! ---\n",
      "2021-12-21 15:23:08,348 (INFO): Modifying stage 'MaxEntropyL1' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = TrainerL1()\n",
    "\n",
    "runner = MaxEntropyL1()\n",
    "runner(operation=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d303ed18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
