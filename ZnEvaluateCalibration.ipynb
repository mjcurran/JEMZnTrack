{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "281ecd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import wideresnet\n",
    "import pdb\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import yaml\n",
    "from zntrack import ZnTrackProject, Node, config, dvc, zn\n",
    "from jemsharedclasses import Base, JEMUtils\n",
    "\n",
    "\n",
    "config.nb_name = \"ZnEvaluateCalibration.ipynb\"\n",
    "project = ZnTrackProject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "474aca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, 10)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a85c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            return t.gather(logits, 1, y[:, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9ce44139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 13:14:36,508 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnEvaluateCalibration.ipynb to script\n",
      "[NbConvertApp] Writing 8766 bytes to ZnEvaluateCalibration.py\n"
     ]
    }
   ],
   "source": [
    "@Node()\n",
    "\n",
    "# Setup parameters\n",
    "class eval_args():\n",
    "    \n",
    "    experiment = dvc.params()\n",
    "    dataset = dvc.params()\n",
    "    n_steps = dvc.params()\n",
    "    width = dvc.params()\n",
    "    depth = dvc.params()\n",
    "    sigma = dvc.params()\n",
    "    data_root = dvc.params()\n",
    "    seed = dvc.params()    \n",
    "    norm = dvc.params()\n",
    "    save_dir = dvc.params()\n",
    "    print_to_log = dvc.params()\n",
    "    \n",
    "    result = zn.metrics()\n",
    "    \n",
    "    src = dvc.deps(Path(\"src\", self.experiment))\n",
    "\n",
    "    \n",
    "    def __call__(self, param_dict):\n",
    "        self.experiment = \"energy_model\"\n",
    "        self.data_root = \"./dataset\" \n",
    "        self.dataset = \"cifar_test\" #, type=str, choices=[\"cifar_train\", \"cifar_test\", \"svhn_test\", \"svhn_train\"], help=\"Dataset to use when running test_clf for classification accuracy\")\n",
    "        self.seed = JEMUtils.get_parameter(\"seed\", 1)\n",
    "        # regularization\n",
    "        self.sigma = 3e-2\n",
    "        # network\n",
    "        self.norm = None #, choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"])\n",
    "        # EBM specific\n",
    "        self.n_steps = 20 # help=\"number of steps of SGLD per iteration, 100 works for short-run, 20 works for PCD\")\n",
    "        self.width = 10 # help=\"WRN width parameter\")\n",
    "        self.depth = 28 # help=\"WRN depth parameter\")        \n",
    "        self.uncond = False # \"store_true\" # help=\"If set, then the EBM is unconditional\")\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.print_to_log = False\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])\n",
    "            \n",
    "    def run(self):\n",
    "        self.result = {\"experiment\": self.experiment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "348e349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Calibration(Base):\n",
    "    \n",
    "    def calibration(self, f, args, device):\n",
    "        transform_test = tr.Compose(\n",
    "            [tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + t.randn_like(x) * args.sigma]\n",
    "        )\n",
    "\n",
    "        def sample(x, n_steps=args.n_steps):\n",
    "            x_k = t.autograd.Variable(x.clone(), requires_grad=True)\n",
    "            # sgld\n",
    "            for k in range(n_steps):\n",
    "                f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "                x_k.data += f_prime + 1e-2 * t.randn_like(x_k)\n",
    "            final_samples = x_k.detach()\n",
    "            return final_samples\n",
    "\n",
    "        if args.dataset == \"cifar_train\":\n",
    "            dset = tv.datasets.CIFAR10(root=args.data_root, transform=transform_test, download=True, train=True)\n",
    "        elif args.dataset == \"cifar_test\":\n",
    "            dset = tv.datasets.CIFAR10(root=args.data_root, transform=transform_test, download=True, train=False)\n",
    "        elif args.dataset == \"svhn_train\":\n",
    "            dset = tv.datasets.SVHN(root=args.data_root, transform=transform_test, download=True, split=\"train\")\n",
    "        else:  # args.dataset == \"svhn_test\":\n",
    "            dset = tv.datasets.SVHN(root=args.data_root, transform=transform_test, download=True, split=\"test\")\n",
    "\n",
    "        dload = DataLoader(dset, batch_size=1, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "        start=0.05\n",
    "        step=.05\n",
    "        num=20\n",
    "\n",
    "        bins=np.arange(0,num)*step+start+ 1e-10\n",
    "        bin_total = np.zeros(20)+1e-5\n",
    "        bin_correct = np.zeros(20)\n",
    "\n",
    "        #energies, corrects, losses, pys, preds = [], [], [], [], []\n",
    "    \n",
    "        for x_p_d, y_p_d in tqdm(dload):\n",
    "            x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "\n",
    "            logits = f.classify(x_p_d).detach().cpu()#.numpy()\n",
    "\n",
    "            py = nn.Softmax()(logits)[0].numpy()#(f.classify(x_p_d)).max(1)[0].detach().cpu().numpy()\n",
    "        \n",
    "            expected = y_p_d[0].detach().cpu().numpy()\n",
    "        \n",
    "            actual = logits.max(1)[1][0].numpy()\n",
    "        \n",
    "            #print(py[expected],expected,actual)\n",
    "        \n",
    "            inds = np.digitize(py[actual], bins)\n",
    "            bin_total[inds] += 1\n",
    "            if actual == expected:\n",
    "                bin_correct[inds] += 1\n",
    "            \n",
    "        #\n",
    "        accu = np.divide(bin_correct,bin_total)\n",
    "        print(\"Bin data\",np.sum(bin_total),accu,bins,bin_total)\n",
    "    \n",
    "        # calc ECE\n",
    "        ECE = 0.0\n",
    "        for i in range(20):\n",
    "            #print(\"accu\",accu[i],(i/20.0 + 0.025),bin_total[i])\n",
    "            ECE += (float(bin_total[i]) / float(np.sum(bin_total))) * abs(accu[i] - (i/20.0 + 0.025))\n",
    "        \n",
    "        print(\"ECE\", ECE)\n",
    "    \n",
    "        # save calibration  in a text file\n",
    "            \n",
    "        pd.DataFrame({'accuracy': accu, 'ECE': ECE}).to_csv(path_or_buf=os.path.join(args.save_dir, args.experiment) + \"_calibration.csv\", index_label=\"index\")\n",
    "\n",
    "        \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        model_cls = F if args.uncond else CCF\n",
    "        f = model_cls(args.depth, args.width, args.norm)\n",
    "        print(f\"loading model from {os.path.join(os.path.join(args.load_path, args.experiment), 'last_ckpt.pt')}\")\n",
    "\n",
    "        # load em up\n",
    "        ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), 'last_ckpt.pt'))\n",
    "        f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "        #replay_buffer = ckpt_dict[\"replay_buffer\"]\n",
    "\n",
    "        f = f.to(device)\n",
    "\n",
    "        # do calibration\n",
    "        self.calibration(f, args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d83c58c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 13:21:00,493 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnEvaluateCalibration.ipynb to script\n",
      "[NbConvertApp] Writing 8519 bytes to ZnEvaluateCalibration.py\n"
     ]
    }
   ],
   "source": [
    "@Node()\n",
    "class EvaluateX:\n",
    "    \n",
    "    args = dvc.deps([eval_args(load=True, name=\"x-entropy_augmented\"), \n",
    "                     eval_args(load=True, name=\"max-entropy-L1_augmented\"), \n",
    "                     eval_args(load=True, name=\"max-entropy-L2_augmented\")])\n",
    "    #arg0: eval_args = dvc.deps(eval_args(name=\"x-entropy_augmented\", load=True))\n",
    "    \n",
    "    calibration: Base = zn.Method()\n",
    "    #result0 = dvc.outs()\n",
    "    result = dvc.outs()\n",
    "    \n",
    "    #def __init__(self):\n",
    "    #    self.result = {}\n",
    "            \n",
    "    def __call__(self, operation):\n",
    "        self.calibration = operation\n",
    "    \n",
    "    def run(self):\n",
    "        for arg in self.args:\n",
    "            self.result += arg.name\n",
    "            self.result += self.calibration.compute(arg)\n",
    "            \n",
    "        #result0 = arg0.experiment\n",
    "        \n",
    "        #result0 += self.calibration.compute(arg0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2407bb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 13:17:57,022 (WARNING): --- Writing new DVC file! ---\n",
      "2021-12-21 13:17:57,873 (INFO): Modifying stage 'x-entropy_augmented' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n",
      "2021-12-21 13:17:57,920 (WARNING): --- Writing new DVC file! ---\n",
      "2021-12-21 13:17:58,700 (INFO): Modifying stage 'max-entropy-L1_augmented' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n",
      "2021-12-21 13:17:58,754 (WARNING): --- Writing new DVC file! ---\n",
      "2021-12-21 13:17:59,466 (INFO): Modifying stage 'max-entropy-L2_augmented' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inline_train_args = {\"load_path\": \"./experiment\", \"experiment\": \"x-entropy_augmented\"}\n",
    "args_train = eval_args(name=\"x-entropy_augmented\")\n",
    "args_train(inline_train_args)\n",
    "\n",
    "inline_L1_args = {\"load_path\": \"./experiment\", \"experiment\": \"max-entropy-L1_augmented\"}\n",
    "args_L1 = eval_args(name=\"max-entropy-L1_augmented\")\n",
    "args_L1(inline_L1_args)\n",
    "\n",
    "inline_L2_args = {\"load_path\": \"./experiment\", \"experiment\": \"max-entropy-L2_augmented\"}\n",
    "args_L2 = eval_args(name=\"max-entropy-L2_augmented\")\n",
    "args_L2(inline_L2_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e2ce05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 13:21:05,168 (ERROR): Can not convert args!\n",
      "2021-12-21 13:21:05,169 (ERROR): Can not convert kwargs!\n",
      "2021-12-21 13:21:05,200 (WARNING): --- Writing new DVC file! ---\n",
      "2021-12-21 13:21:05,201 (WARNING): Found outs with value None that cannot be processed - skipping it.\n",
      "2021-12-21 13:21:05,908 (INFO): Modifying stage 'EvaluateX' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cal = Calibration()\n",
    "eva = EvaluateX()\n",
    "eva(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a46da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stage 'train_argsL1':\n",
      "> python3 -c \"from src.train_argsL1 import train_argsL1; train_argsL1(load=True, name='train_argsL1').run()\" \n",
      "Updating lock file 'dvc.lock'\n",
      "\n",
      "Running stage 'MaxEntropyL1':\n",
      "> python3 -c \"from src.MaxEntropyL1 import MaxEntropyL1; MaxEntropyL1(load=True, name='MaxEntropyL1').run()\" \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crc/python/jem_clean/__pypackages__/3.9/lib/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "project.repro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac710e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
