{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c523ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "#import wideresnet\n",
    "from jemsharedclasses import Base, JEMUtils, FTrain, DataSubset\n",
    "import pdb\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from zntrack import ZnTrackProject, Node, config, dvc, zn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from zntrack.metadata import TimeIt\n",
    "from train_args import train_args\n",
    "from dvc.api import make_checkpoint\n",
    "\n",
    "\n",
    "config.nb_name = \"Znv3MaxEntropyL2.ipynb\"\n",
    "project = ZnTrackProject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac10d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerL2(Base):\n",
    "    \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "        \n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = FTrain(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path and os.path.exists(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt')):\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            #ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt'))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "    \n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # get logits for calculations\n",
    "                logits = f.classify(x_lab)\n",
    "\n",
    "                ####################################################\n",
    "                # Maximize entropy by assuming equal probabilities #\n",
    "                ####################################################\n",
    "                energy = logits.logsumexp(dim=1, keepdim=False)\n",
    "            \n",
    "                e_mean = t.mean(energy)\n",
    "                #print('Energy shape',energy.size())\n",
    "            \n",
    "                energy_loss = t.sum((e_mean - energy)**2)\n",
    "            \n",
    "                L += energy_loss\n",
    "            \n",
    "                ######################################\n",
    "                # normal cross entropy loss function #\n",
    "                ######################################\n",
    "                # maximize log p(y | x)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergwence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "                with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "                    json.dump(scores, outfile)\n",
    "                make_checkpoint()\n",
    "\n",
    "            \n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "\n",
    "        # write stats\n",
    "        #with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "        #    json.dump(scores, outfile)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5e419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEntropyL2(Node):\n",
    "    params: train_args = zn.Method()\n",
    "    metrics: Path = dvc.metrics_no_cache()\n",
    "    model: Path = dvc.outs()\n",
    "    # manually add checkpoint: true to the outs in dvc.yaml\n",
    "    \n",
    "    operation: Base = zn.Method()\n",
    "            \n",
    "    def __init__(self, params: train_args = None, operation: Base = None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params = params\n",
    "        self.operation = operation\n",
    "            \n",
    "        if not self.is_loaded:\n",
    "            self.params = train_args(experiment='max-entropy-L2_augmented')\n",
    "        \n",
    "        #Make sure this path is available at the time the dvc stage is declared or it will error out\n",
    "        if params != None and not os.path.exists(os.path.join(params.save_dir, params.experiment)):\n",
    "            os.makedirs(os.path.join(params.save_dir, params.experiment))\n",
    "            \n",
    "        self.metrics = Path(os.path.join(self.params.save_dir, self.params.experiment) + '_scores.json')\n",
    "        self.model = Path(os.path.join(os.path.join(self.params.save_dir, self.params.experiment), f'ckpt_{self.params.experiment}.pt'))\n",
    "    \n",
    "\n",
    "    def run(self):\n",
    "        scores = self.operation.compute(self.params)\n",
    "        with open(self.metrics, 'w') as outfile:\n",
    "            json.dump(scores, outfile)\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7229e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-26 14:23:05,329 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n",
      "2022-01-26 14:23:05,331 (WARNING): Converting Znv3MaxEntropyL2.ipynb to file MaxEntropyL2.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Znv3MaxEntropyL2.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-26 14:23:07,286 (ERROR): Can not convert args!\n",
      "2022-01-26 14:23:07,287 (ERROR): Can not convert kwargs!\n",
      "2022-01-26 14:23:07,319 (WARNING): --- Writing new DVC file! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 8623 bytes to Znv3MaxEntropyL2.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-26 14:23:08,279 (INFO): Modifying stage 'MaxEntropyL2' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = train_args(lr=.0001, experiment='max-entropy-L2_augmented', load_path='./experiment')\n",
    "trainer = TrainerL2()\n",
    "\n",
    "MaxEntropyL2(params=params, operation = trainer).write_graph(no_exec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b7e74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
