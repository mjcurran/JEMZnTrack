{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a2c276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import wideresnet\n",
    "import pdb\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from zntrack import ZnTrackProject, Node, config, dvc, zn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from zntrack.metadata import TimeIt\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "config.nb_name = \"Znv3JEMProject.ipynb\"\n",
    "project = ZnTrackProject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20cc3b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
      "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
      "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
      "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
      "\u001b[33mhint: \u001b[m\n",
      "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
      "Initialized empty Git repository in /Users/crc/python/zntrack03/.git/\n",
      "2022-01-19 10:19:32,392 (INFO): Setting up GIT/DVC repository.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "!git init\n",
    "project.create_dvc_repository()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724da358",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class train_args:\n",
    "    norm: str = None\n",
    "    load_path: str = \"./experiment\"\n",
    "    experiment: str = \"energy-models\"\n",
    "    dataset: str = \"./dataset\"\n",
    "    n_classes: int = 10\n",
    "    n_steps: int = 20\n",
    "    width: int = 10\n",
    "    depth: int = 28\n",
    "    sigma: float = 0.3\n",
    "    data_root: str = \"./dataset\" \n",
    "    seed: int = 123456\n",
    "    lr: float = 1e-4\n",
    "    clf_only: bool = False\n",
    "    labels_per_class: int = -1\n",
    "    batch_size: int = 64\n",
    "    n_epochs: int = 10\n",
    "    dropout_rate: float = 0.0\n",
    "    weight_decay: float = 0.0\n",
    "    save_dir: str = \"./experiment\"\n",
    "    ckpt_every: int = 1\n",
    "    eval_every: int = 11\n",
    "    print_every: int = 100\n",
    "    print_to_log: bool = False\n",
    "    n_valid: int = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76e3c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a base for the Node compute functions, to split off the actual work from the dvc control flow\n",
    "class Base:\n",
    "    def compute(self, inp):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d73332",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JEMUtils:\n",
    "    \n",
    "    \n",
    "    # various utilities\n",
    "    @staticmethod\n",
    "    def cycle(loader):\n",
    "        while True:\n",
    "            for data in loader:\n",
    "                yield data\n",
    "                \n",
    "                \n",
    "    # calculate loss and accuracy for periodic printout\n",
    "    \n",
    "    @staticmethod\n",
    "    def eval_classification(f, dload, device):\n",
    "        corrects, losses = [], []\n",
    "        for x_p_d, y_p_d in dload:\n",
    "            x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "            logits = f.classify(x_p_d)\n",
    "            loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().numpy()\n",
    "            losses.extend(loss)\n",
    "            correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "            corrects.extend(correct)\n",
    "        loss = np.mean(losses)\n",
    "        correct = np.mean(corrects)\n",
    "        return correct, loss\n",
    "    \n",
    "    \n",
    "    # save checkpoint data\n",
    "    \n",
    "    @staticmethod\n",
    "    def checkpoint(f, opt, epoch_no, tag, args, device):\n",
    "        f.cpu()\n",
    "        ckpt_dict = {\n",
    "            \"model_state_dict\": f.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'epoch': epoch_no,\n",
    "            #\"replay_buffer\": buffer\n",
    "        }\n",
    "        t.save(ckpt_dict, os.path.join(os.path.join(args.save_dir, args.experiment), tag))\n",
    "        f.to(device)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def get_parameter(name, default):\n",
    "        params = yaml.safe_load(open(\"params.yaml\"))\n",
    "        to_search = params\n",
    "        for part in name.split(\".\"):\n",
    "            result = to_search.get(part)\n",
    "            if result == None:\n",
    "                return default\n",
    "            to_search = result\n",
    "        return to_search\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_data(args):\n",
    "        im_sz = 32\n",
    "        \n",
    "        \n",
    "        #global transform_train\n",
    "        # the GaussianBlur is roughly equivalent to the lambda functions here\n",
    "        # but the lambda functions aren't serializable for multi-processing\n",
    "        # torchvision.transforms documentation state to not use lambda functions as well\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.RandomHorizontalFlip(),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             tr.GaussianBlur(kernel_size=(5, 5), sigma=(args.sigma, args.sigma * 2))]\n",
    "             #lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "        #global transform_test\n",
    "        transform_test = tr.Compose(\n",
    "            [tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             tr.GaussianBlur(kernel_size=(5, 5), sigma=(args.sigma, args.sigma * 2))]\n",
    "             #lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        def dataset_fn(train, transform):\n",
    "            return tv.datasets.CIFAR10(root='./dataset', transform=transform, download=True, train=train)\n",
    "\n",
    "        # get all training inds\n",
    "        #global full_train\n",
    "        full_train = dataset_fn(train=True, transform=transform_train)\n",
    "        all_inds = list(range(len(full_train)))\n",
    "        # set seed\n",
    "        np.random.seed(args.seed)\n",
    "        # shuffle\n",
    "        np.random.shuffle(all_inds)\n",
    "        # seperate out validation set\n",
    "        if args.n_valid is not None:\n",
    "            valid_inds, train_inds = all_inds[:args.n_valid], all_inds[args.n_valid:]\n",
    "        else:\n",
    "            valid_inds, train_inds = [], all_inds\n",
    "        train_inds = np.array(train_inds)\n",
    "        train_labeled_inds = []\n",
    "        other_inds = []\n",
    "        train_labels = np.array([full_train[ind][1] for ind in train_inds])\n",
    "        if args.labels_per_class > 0:\n",
    "            for i in range(args.n_classes):\n",
    "                print(i)\n",
    "                train_labeled_inds.extend(train_inds[train_labels == i][:args.labels_per_class])\n",
    "                other_inds.extend(train_inds[train_labels == i][args.labels_per_class:])\n",
    "        else:\n",
    "            train_labeled_inds = train_inds\n",
    "\n",
    "        dset_train = DataSubset(\n",
    "            dataset_fn(train=True, transform=transform_train),\n",
    "            inds=train_inds)\n",
    "        dset_train_labeled = DataSubset(\n",
    "            dataset_fn(train=True, transform=transform_train),\n",
    "            inds=train_labeled_inds)\n",
    "        dset_valid = DataSubset(\n",
    "            dataset_fn(train=True, transform=transform_test),\n",
    "            inds=valid_inds)\n",
    "        \n",
    "        dload_train = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "        dload_train_labeled = DataLoader(dset_train_labeled, batch_size=args.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "        dload_train_labeled = JEMUtils.cycle(dload_train_labeled)\n",
    "        dset_test = dataset_fn(train=False, transform=transform_test)\n",
    "        dload_valid = DataLoader(dset_valid, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "        dload_test = DataLoader(dset_test, batch_size=100, shuffle=False, num_workers=4, drop_last=False)\n",
    "        return dload_train, dload_train_labeled, dload_valid, dload_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "635626ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random subset of data\n",
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0baadbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Wide_ResNet\n",
    "# Uses The Google Research Authors, file wideresnet.py\n",
    "class FTrain(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(FTrain, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm, dropout_rate=dropout_rate)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, n_classes)\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "481875ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the operations from train.ipynb and track in dvc\n",
    "#dependency is train_args stage with default name\n",
    "#outs is the path to the last_ckpt.pt model file, which serves as a dependency to the evaluation stage\n",
    "\n",
    "class XEntropyAugmented(Node):\n",
    "    \n",
    "    params: train_args = zn.Method()\n",
    "    model: Path = dvc.outs()\n",
    "    metrics: Path = dvc.metrics_no_cache() \n",
    "    \n",
    "    def __init__(self, params: train_args = None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params = params\n",
    "        if params != None and not os.path.exists(os.path.join(params.save_dir, params.experiment)):\n",
    "            os.makedirs(os.path.join(params.save_dir, params.experiment))\n",
    "            \n",
    "        if not self.is_loaded:\n",
    "            self.params = train_args(experiment='x-entropy_augmented')\n",
    "        \n",
    "        self.metrics = Path(os.path.join(self.params.save_dir, self.params.experiment) + '_scores.json')\n",
    "        self.model = Path(os.path.join(os.path.join(self.params.save_dir, self.params.experiment), f'ckpt_{self.params.experiment}.pt'))\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        scores = self.compute(self.params)\n",
    "        with open(self.metrics, 'w') as outfile:\n",
    "            json.dump(scores, outfile)\n",
    "        \n",
    "    \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        \n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = FTrain(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path and os.path.exists(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt')):\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            #ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt'))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "    \n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # normal cross entropy loss function\n",
    "                # maximize log p(y | x)\n",
    "                logits = f.classify(x_lab)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            #changing to always use the same file name for each experiment and use the dvc checkpoint\n",
    "            # to cache distinct copies when needed\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                #JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "                JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "                with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "                    json.dump(scores, outfile)\n",
    "                make_checkpoint()\n",
    "\n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            #JEMUtils.checkpoint(f, optim, epoch, \"last_ckpt.pt\", args, device)\n",
    "            JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "\n",
    "        # write stats\n",
    "        #with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "        #    json.dump(scores, outfile)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d757e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:53:25,661 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n",
      "2022-01-19 16:53:25,663 (WARNING): Converting Znv3JEMProject.ipynb to file XEntropyAugmented.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Znv3JEMProject.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:53:27,298 (WARNING): --- Writing new DVC file! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 37567 bytes to Znv3JEMProject.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:53:29,217 (INFO): Modifying stage 'XEntropyAugmented' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "XEntropyAugmented(params = train_args(experiment='x-entropy_augmented', lr=.0001, load_path='./experiment')).write_graph(no_exec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c70be162",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaxEntropyL1(Node):\n",
    "    params: train_args = zn.Method()\n",
    "    metrics: Path = dvc.metrics_no_cache()\n",
    "    model: Path = dvc.outs()\n",
    "    # manually add checkpoint: true to the outs in dvc.yaml\n",
    "            \n",
    "    def __init__(self, params: train_args = None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params = params\n",
    "        #Make sure this path is available at the time the dvc stage is declared or it will error out\n",
    "        if params != None and not os.path.exists(os.path.join(params.save_dir, params.experiment)):\n",
    "            os.makedirs(os.path.join(params.save_dir, params.experiment))\n",
    "            \n",
    "        if not self.is_loaded:\n",
    "            self.params = train_args(experiment='max-entropy-L1_augmented')\n",
    "            \n",
    "        self.metrics = Path(os.path.join(self.params.save_dir, self.params.experiment) + '_scores.json')\n",
    "        self.model = Path(os.path.join(os.path.join(self.params.save_dir, self.params.experiment), f'ckpt_{self.params.experiment}.pt'))\n",
    "    \n",
    "\n",
    "    def run(self):\n",
    "        scores = self.compute(self.params)\n",
    "        with open(self.metrics, 'w') as outfile:\n",
    "            json.dump(scores, outfile)\n",
    "            \n",
    "            \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "        \n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = FTrain(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path and os.path.exists(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt')):\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            #ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt'))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # get logits for calculations\n",
    "                logits = f.classify(x_lab)\n",
    "\n",
    "                ####################################################\n",
    "                # Maximize entropy by assuming equal probabilities #\n",
    "                ####################################################\n",
    "                energy = logits.logsumexp(dim=1, keepdim=False)\n",
    "            \n",
    "                e_mean = t.mean(energy)\n",
    "                #print('Energy shape',energy.size())\n",
    "            \n",
    "                energy_loss = t.sum(t.abs(e_mean - energy))\n",
    "            \n",
    "                L += energy_loss\n",
    "            \n",
    "                ######################################\n",
    "                # normal cross entropy loss function #\n",
    "                ######################################\n",
    "                # maximize log p(y | x)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergwence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "                with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "                    json.dump(scores, outfile)\n",
    "                make_checkpoint()\n",
    "            \n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "\n",
    "        # write stats\n",
    "        #with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "        #    json.dump(scores, outfile)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "719e6af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:53:41,211 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n",
      "2022-01-19 16:53:41,212 (WARNING): Converting Znv3JEMProject.ipynb to file MaxEntropyL1.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Znv3JEMProject.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:53:42,914 (WARNING): --- Writing new DVC file! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 37567 bytes to Znv3JEMProject.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:53:44,716 (INFO): Modifying stage 'MaxEntropyL1' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MaxEntropyL1(params=train_args(lr=.0001, experiment='max-entropy-L1_augmented', load_path='./experiment')).write_graph(no_exec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4ccfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEntropyL2(Node):\n",
    "    params: train_args = zn.Method()\n",
    "    metrics: Path = dvc.metrics_no_cache()\n",
    "    model: Path = dvc.outs()\n",
    "    # manually add checkpoint: true to the outs in dvc.yaml\n",
    "            \n",
    "    def __init__(self, params: train_args = None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params = params\n",
    "        #Make sure this path is available at the time the dvc stage is declared or it will error out\n",
    "        if params != None and not os.path.exists(os.path.join(params.save_dir, params.experiment)):\n",
    "            os.makedirs(os.path.join(params.save_dir, params.experiment))\n",
    "            \n",
    "        if not self.is_loaded:\n",
    "            self.params = train_args(experiment='max-entropy-L2_augmented')\n",
    "            \n",
    "        self.metrics = Path(os.path.join(self.params.save_dir, self.params.experiment) + '_scores.json')\n",
    "        self.model = Path(os.path.join(os.path.join(self.params.save_dir, self.params.experiment), f'ckpt_{self.params.experiment}.pt'))\n",
    "    \n",
    "\n",
    "    def run(self):\n",
    "        scores = self.compute(self.params)\n",
    "        with open(self.metrics, 'w') as outfile:\n",
    "            json.dump(scores, outfile)\n",
    "            \n",
    "            \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "        \n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = FTrain(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path and os.path.exists(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt')):\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            #ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt'))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "    \n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # get logits for calculations\n",
    "                logits = f.classify(x_lab)\n",
    "\n",
    "                ####################################################\n",
    "                # Maximize entropy by assuming equal probabilities #\n",
    "                ####################################################\n",
    "                energy = logits.logsumexp(dim=1, keepdim=False)\n",
    "            \n",
    "                e_mean = t.mean(energy)\n",
    "                #print('Energy shape',energy.size())\n",
    "            \n",
    "                energy_loss = t.sum((e_mean - energy)**2)\n",
    "            \n",
    "                L += energy_loss\n",
    "            \n",
    "                ######################################\n",
    "                # normal cross entropy loss function #\n",
    "                ######################################\n",
    "                # maximize log p(y | x)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergwence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "                with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "                    json.dump(scores, outfile)\n",
    "                make_checkpoint()\n",
    "\n",
    "            \n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "\n",
    "        # write stats\n",
    "        #with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "        #    json.dump(scores, outfile)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eea52cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:53:51,875 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n",
      "2022-01-19 16:53:51,877 (WARNING): Converting Znv3JEMProject.ipynb to file MaxEntropyL2.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Znv3JEMProject.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:53:53,605 (WARNING): --- Writing new DVC file! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 37567 bytes to Znv3JEMProject.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:53:55,560 (INFO): Modifying stage 'MaxEntropyL2' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MaxEntropyL2(params=train_args(lr=.0001, experiment='max-entropy-L2_augmented', load_path='./experiment')).write_graph(no_exec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "917a001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, 10)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        penult_z = self.f(x)\n",
    "        return self.energy_output(penult_z).squeeze()\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2d301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CCF(F):\n",
    "    def __init__(self, depth=28, width=2, norm=None):\n",
    "        super(CCF, self).__init__(depth, width, norm=norm)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        logits = self.classify(x)\n",
    "        if y is None:\n",
    "            return logits.logsumexp(1)\n",
    "        else:\n",
    "            return t.gather(logits, 1, y[:, None])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5761659",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class eval_args():\n",
    "    \n",
    "    experiment: str = \"energy_model\"\n",
    "    dataset: str = \"cifar_test\"\n",
    "    n_steps: int = 20\n",
    "    width: int = 10\n",
    "    depth: int = 28\n",
    "    sigma: float = .03\n",
    "    data_root: str = \"./dataset\"\n",
    "    seed: int = 123456\n",
    "    norm: str = None\n",
    "    save_dir: str = \"./experiment\"\n",
    "    print_to_log: bool = False\n",
    "    uncond: bool = False\n",
    "    load_path: str = \"./experiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5d55ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateX(Node):\n",
    "    \n",
    "    #from the DVC docs:  \"Stage dependencies can be any file or directory\"\n",
    "    # so the eval_args stages have to output something in order to be used as deps here\n",
    "    # so we use the metrics files like:  nodes/x-entropy_augmented/metrics_no_cache.json\n",
    "    #args = dvc.deps([eval_args(load=True, name=\"x-entropy_augmented\"), \n",
    "    #                 eval_args(load=True, name=\"max-entropy-L1_augmented\"), \n",
    "    #                 eval_args(load=True, name=\"max-entropy-L2_augmented\")])\n",
    "\n",
    "    \n",
    "    #models = dvc.deps([XEntropyAugmented(load=True), MaxEntropyL1(load=True), MaxEntropyL2(load=True)])\n",
    "\n",
    "    models = dvc.deps([XEntropyAugmented.load(), MaxEntropyL1.load(), MaxEntropyL2.load()])\n",
    "    params: eval_args = zn.Method()\n",
    "        \n",
    "    # add plots to dvc tracking\n",
    "    # this would be better if the paths could be defined by the passed args, but can't see how to \n",
    "    plot0: Path = dvc.plots_no_cache(\"./experiment/x-entropy_augmented_calibration.csv\")\n",
    "    plot1: Path = dvc.plots_no_cache(\"./experiment/max-entropy-L1_augmented_calibration.csv\")\n",
    "    plot2: Path = dvc.plots_no_cache(\"./experiment/max-entropy-L2_augmented_calibration.csv\")\n",
    "    #manually added template: confidence to the plots in dvc.yaml\n",
    "    \n",
    "    def __init__(self, params: eval_args = None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params = params\n",
    "        if not self.is_loaded:\n",
    "            self.params = eval_args()\n",
    "    \n",
    "    \n",
    "\n",
    "    def run(self):\n",
    "        for arg in self.models:\n",
    "            self.compute(arg, self.params)\n",
    "            #with open('./experiment/joint_energy_models_scores.json', 'a') as outfile:\n",
    "            #    json.dump(scores, outfile)\n",
    "            \n",
    "            \n",
    "    def compute(self, inp, params):\n",
    "        args = inp\n",
    "        if not os.path.exists(params.save_dir):\n",
    "            os.makedirs(params.save_dir)\n",
    "        \n",
    "        if params.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(params.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        if not os.path.exists(os.path.join(params.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(params.save_dir, args.experiment))\n",
    "\n",
    "        t.manual_seed(params.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(params.seed)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        model_cls = F if params.uncond else CCF\n",
    "        f = model_cls(params.depth, params.width, params.norm)\n",
    "        print(f\"loading model from {os.path.join(os.path.join(params.load_path, args.experiment), 'last_ckpt.pt')}\")\n",
    "\n",
    "        # load em up\n",
    "        ckpt_dict = t.load(os.path.join(os.path.join(params.load_path, args.experiment), 'last_ckpt.pt'))\n",
    "        f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "        #replay_buffer = ckpt_dict[\"replay_buffer\"]\n",
    "\n",
    "        f = f.to(device)\n",
    "\n",
    "        # do calibration\n",
    "        resultfile = self.calibration(f, args, params, device)\n",
    "        return resultfile\n",
    "    \n",
    "    \n",
    "    def calibration(self, f, args, params, device):\n",
    "        transform_test = tr.Compose(\n",
    "            [tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             lambda x: x + t.randn_like(x) * params.sigma]\n",
    "        )\n",
    "\n",
    "        def sample(x, n_steps=params.n_steps):\n",
    "            x_k = t.autograd.Variable(x.clone(), requires_grad=True)\n",
    "            # sgld\n",
    "            for k in range(n_steps):\n",
    "                f_prime = t.autograd.grad(f(x_k).sum(), [x_k], retain_graph=True)[0]\n",
    "                x_k.data += f_prime + 1e-2 * t.randn_like(x_k)\n",
    "            final_samples = x_k.detach()\n",
    "            return final_samples\n",
    "\n",
    "        if params.dataset == \"cifar_train\":\n",
    "            dset = tv.datasets.CIFAR10(root=params.data_root, transform=transform_test, download=True, train=True)\n",
    "        elif params.dataset == \"cifar_test\":\n",
    "            dset = tv.datasets.CIFAR10(root=params.data_root, transform=transform_test, download=True, train=False)\n",
    "        elif params.dataset == \"svhn_train\":\n",
    "            dset = tv.datasets.SVHN(root=params.data_root, transform=transform_test, download=True, split=\"train\")\n",
    "        else:  # args.dataset == \"svhn_test\":\n",
    "            dset = tv.datasets.SVHN(root=params.data_root, transform=transform_test, download=True, split=\"test\")\n",
    "\n",
    "        dload = DataLoader(dset, batch_size=1, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "        start=0.05\n",
    "        step=.05\n",
    "        num=20\n",
    "\n",
    "        bins=np.arange(0,num)*step+start+ 1e-10\n",
    "        bin_total = np.zeros(20)+1e-5\n",
    "        bin_correct = np.zeros(20)\n",
    "\n",
    "        #energies, corrects, losses, pys, preds = [], [], [], [], []\n",
    "    \n",
    "        for x_p_d, y_p_d in tqdm(dload):\n",
    "            x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "\n",
    "            logits = f.classify(x_p_d).detach().cpu()#.numpy()\n",
    "\n",
    "            py = nn.Softmax()(logits)[0].numpy()#(f.classify(x_p_d)).max(1)[0].detach().cpu().numpy()\n",
    "        \n",
    "            expected = y_p_d[0].detach().cpu().numpy()\n",
    "        \n",
    "            actual = logits.max(1)[1][0].numpy()\n",
    "        \n",
    "            #print(py[expected],expected,actual)\n",
    "        \n",
    "            inds = np.digitize(py[actual], bins)\n",
    "            bin_total[inds] += 1\n",
    "            if actual == expected:\n",
    "                bin_correct[inds] += 1\n",
    "            \n",
    "        #\n",
    "        accu = np.divide(bin_correct,bin_total)\n",
    "        print(\"Bin data\",np.sum(bin_total),accu,bins,bin_total)\n",
    "    \n",
    "        # calc ECE\n",
    "        ECE = 0.0\n",
    "        for i in range(20):\n",
    "            #print(\"accu\",accu[i],(i/20.0 + 0.025),bin_total[i])\n",
    "            ECE += (float(bin_total[i]) / float(np.sum(bin_total))) * abs(accu[i] - (i/20.0 + 0.025))\n",
    "        \n",
    "        print(\"ECE\", ECE)\n",
    "    \n",
    "        # save calibration  in a text file\n",
    "            \n",
    "        pd.DataFrame({'accuracy': accu, 'ECE': ECE}).to_csv(path_or_buf=os.path.join(params.save_dir, args.experiment) + \"_calibration.csv\", index_label=\"index\")\n",
    "        outputcsv = os.path.join(params.save_dir, args.experiment) + \"_calibration.csv\"\n",
    "        return outputcsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "148d548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:46:02,992 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n",
      "2022-01-19 16:46:02,993 (WARNING): Converting Znv3JEMProject.ipynb to file EvaluateX.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Znv3JEMProject.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:46:04,880 (WARNING): --- Writing new DVC file! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 37537 bytes to Znv3JEMProject.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-19 16:46:07,241 (INFO): Adding stage 'EvaluateX' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EvaluateX(params=eval_args()).write_graph(no_exec=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f4311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stage 'XEntropyAugmented':\n",
      "> python3 -c \"from src.XEntropyAugmented import XEntropyAugmented; XEntropyAugmented.load(name='XEntropyAugmented').run_and_save()\" \n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170499072it [00:12, 13489178.49it/s]                               \n"
     ]
    }
   ],
   "source": [
    "project.repro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b920c57a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
