{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4122184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "#import wideresnet\n",
    "from jemsharedclasses import Base, JEMUtils, FTrain, DataSubset\n",
    "import pdb\n",
    "import json\n",
    "#from matplotlib import pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from zntrack import ZnTrackProject, Node, config, dvc, zn\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from zntrack.metadata import TimeIt\n",
    "from train_args import train_args\n",
    "\n",
    "\n",
    "config.nb_name = \"Znv3Train.ipynb\"\n",
    "project = ZnTrackProject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86623668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(Base):\n",
    "    \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        \n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = FTrain(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path and os.path.exists(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt')):\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            #ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt'))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "    \n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # normal cross entropy loss function\n",
    "                # maximize log p(y | x)\n",
    "                logits = f.classify(x_lab)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            #changing to always use the same file name for each experiment and use the dvc checkpoint\n",
    "            # to cache distinct copies when needed\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                #JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "                JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "                with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "                    json.dump(scores, outfile)\n",
    "                make_checkpoint()\n",
    "\n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            #JEMUtils.checkpoint(f, optim, epoch, \"last_ckpt.pt\", args, device)\n",
    "            JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "\n",
    "        # write stats\n",
    "        #with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "        #    json.dump(scores, outfile)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a29ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do the operations from train.ipynb and track in dvc\n",
    "#dependency is train_args stage with default name\n",
    "#outs is the path to the last_ckpt.pt model file, which serves as a dependency to the evaluation stage\n",
    "\n",
    "class XEntropyAugmented(Node):\n",
    "    \n",
    "    params: train_args = zn.Method()\n",
    "    model: Path = dvc.outs(\"./experiment/x-entropy_augmented/ckpt_x-entropy_augmented.pt\")\n",
    "    metrics: Path = dvc.metrics_no_cache(\"./experiment/x-entropy_augmented_scores.json\") \n",
    "    \n",
    "    def __init__(self, params: train_args = None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.params = params\n",
    "            \n",
    "        if not self.is_loaded:\n",
    "            self.params = train_args(experiment='x-entropy_augmented')\n",
    "        \n",
    "        if params != None and not os.path.exists(os.path.join(params.save_dir, params.experiment)):\n",
    "            os.makedirs(os.path.join(params.save_dir, params.experiment))\n",
    "            \n",
    "        #self.metrics = Path(os.path.join(self.params.save_dir, self.params.experiment) + '_scores.json')\n",
    "        #self.model = Path(os.path.join(os.path.join(self.params.save_dir, self.params.experiment), f'ckpt_{self.params.experiment}.pt'))\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        scores = self.compute(self.params)\n",
    "        with open(self.metrics, 'w') as outfile:\n",
    "            json.dump(scores, outfile)\n",
    "        \n",
    "    \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        \n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = FTrain(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path and os.path.exists(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt')):\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            #ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            ckpt_dict = t.load(os.path.join(os.path.join(args.load_path, args.experiment), f'ckpt_{args.experiment}.pt'))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "    \n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # normal cross entropy loss function\n",
    "                # maximize log p(y | x)\n",
    "                logits = f.classify(x_lab)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            #changing to always use the same file name for each experiment and use the dvc checkpoint\n",
    "            # to cache distinct copies when needed\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                #JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "                JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "                with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "                    json.dump(scores, outfile)\n",
    "                make_checkpoint()\n",
    "\n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            #JEMUtils.checkpoint(f, optim, epoch, \"last_ckpt.pt\", args, device)\n",
    "            JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{args.experiment}.pt', args, device)\n",
    "\n",
    "        # write stats\n",
    "        #with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "        #    json.dump(scores, outfile)\n",
    "            \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ce1007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-25 16:04:59,610 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n",
      "2022-01-25 16:04:59,612 (WARNING): Converting Znv3Train.ipynb to file XEntropyAugmented.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Znv3Train.ipynb to script\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-25 16:05:01,952 (WARNING): --- Writing new DVC file! ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Writing 14210 bytes to Znv3Train.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-25 16:05:04,347 (INFO): Modifying stage 'XEntropyAugmented' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = train_args(experiment='x-entropy_augmented', lr=.0001, load_path='./experiment')\n",
    "#trainer = Trainer()\n",
    "XEntropyAugmented(params = params).write_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bba8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stage 'XEntropyAugmented':\n",
      "> python3 -c \"from src.XEntropyAugmented import XEntropyAugmented; XEntropyAugmented.load(name='XEntropyAugmented').run_and_save()\" \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crc/python/03JEMZnTrack/__pypackages__/3.9/lib/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "| Wide-Resnet 28x10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/crc/python/03JEMZnTrack/__pypackages__/3.9/lib/zntrack/core/base.py\", line 107, in run_and_save\n",
      "    self.run()\n",
      "  File \"/Users/crc/python/03JEMZnTrack/src/XEntropyAugmented.py\", line 189, in run\n",
      "    scores = self.compute(self.params)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/src/XEntropyAugmented.py\", line 244, in compute\n",
      "    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/jemsharedclasses.py\", line 94, in eval_classification\n",
      "    logits = f.classify(x_p_d)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/jemsharedclasses.py\", line 38, in classify\n",
      "    penult_z = self.f(x)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/__pypackages__/3.9/lib/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/wideresnet.py\", line 116, in forward\n",
      "    out = self.layer1(out)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/__pypackages__/3.9/lib/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/__pypackages__/3.9/lib/torch/nn/modules/container.py\", line 141, in forward\n",
      "    input = module(input)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/__pypackages__/3.9/lib/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/wideresnet.py\", line 60, in forward\n",
      "    out = self.conv2(self.lrelu(self.bn2(out)))\n",
      "  File \"/Users/crc/python/03JEMZnTrack/__pypackages__/3.9/lib/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/__pypackages__/3.9/lib/torch/nn/modules/conv.py\", line 446, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "  File \"/Users/crc/python/03JEMZnTrack/__pypackages__/3.9/lib/torch/nn/modules/conv.py\", line 442, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepro\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python/03JEMZnTrack/__pypackages__/3.9/lib/zntrack/project/zntrack_project.py:113\u001b[0m, in \u001b[0;36mZnTrackProject.repro\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepro\u001b[39m():\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124;03m\"\"\"Run dvc repro\"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdvc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrepro\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/subprocess.py:368\u001b[0m, in \u001b[0;36mcheck_call\u001b[0;34m(*popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_call\u001b[39m(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Run command with arguments.  Wait for command to complete.  If\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m    the exit code was zero then return, otherwise raise\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    CalledProcessError.  The CalledProcessError object will have the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    check_call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[1;32m    370\u001b[0m         cmd \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/subprocess.py:351\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 351\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:  \u001b[38;5;66;03m# Including KeyboardInterrupt, wait handled that.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m         p\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/subprocess.py:1189\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 1917\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   1919\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/subprocess.py:1875\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1873\u001b[0m \u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   1874\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1875\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1876\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   1877\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   1879\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   1880\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "project.repro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00195ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
