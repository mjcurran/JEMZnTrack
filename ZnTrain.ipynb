{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24164619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch as t, torch.nn as nn, torch.nn.functional as tnnF, torch.distributions as tdist\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision as tv, torchvision.transforms as tr\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import wideresnet # from The Google Research Authors\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd \n",
    "from zntrack import ZnTrackProject, Node, config, dvc, zn\n",
    "\n",
    "config.nb_name = \"ZnTrain.ipynb\"\n",
    "project = ZnTrackProject()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53cac9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 13:35:29,585 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnTrain.ipynb to script\n",
      "[NbConvertApp] Writing 15202 bytes to ZnTrain.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup parameters\n",
    "# defaults for paper\n",
    "# --lr .0001 --dataset cifar10 --optimizer adam --p_x_weight 1.0 --p_y_given_x_weight 1.0 \n",
    "# --p_x_y_weight 0.0 --sigma .03 --width 10 --depth 28 --save_dir /YOUR/SAVE/DIR \n",
    "# --plot_uncond --warmup_iters 1000\n",
    "#\n",
    "\n",
    "@Node()\n",
    "class train_args():\n",
    "    # define params\n",
    "    # this will write them to params.yaml\n",
    "    experiment = dvc.params()\n",
    "    dataset = dvc.params()\n",
    "    n_classes = dvc.params()\n",
    "    n_steps = dvc.params()\n",
    "    width = dvc.params()\n",
    "    depth = dvc.params()\n",
    "    sigma = dvc.params()\n",
    "    data_root = dvc.params()\n",
    "    seed = dvc.params()\n",
    "    lr = dvc.params()\n",
    "    clf_only = dvc.params()\n",
    "    labels_per_class = dvc.params()\n",
    "    batch_size = dvc.params()\n",
    "    n_epochs = dvc.params()\n",
    "    dropout_rate = dvc.params()\n",
    "    weight_decay = dvc.params()\n",
    "    norm = dvc.params()\n",
    "    save_dir = dvc.params()\n",
    "    ckpt_every = dvc.params()\n",
    "    eval_every = dvc.params()\n",
    "    print_every = dvc.params()\n",
    "    load_path = dvc.params()\n",
    "    print_to_log = dvc.params()\n",
    "    n_valid = dvc.params()\n",
    "    \n",
    "    result = zn.metrics()\n",
    "    \n",
    "    def __call__(self, param_dict):\n",
    "        # set defaults\n",
    "        self.experiment = \"energy_model\"\n",
    "        self.dataset = \"cifar10\"\n",
    "        self.n_classes = 10\n",
    "        self.n_steps = 20\n",
    "        self.width = 10 # wide-resnet widen_factor\n",
    "        self.depth = 28  # wide-resnet depth\n",
    "        self.sigma = .03 # image transformation\n",
    "        self.data_root = \"./dataset\" \n",
    "        self.seed = JEMUtils.get_parameter(\"seed\", 1)\n",
    "        # optimization\n",
    "        self.lr = 1e-4\n",
    "        self.clf_only = False #action=\"store_true\", help=\"If set, then only train the classifier\")\n",
    "        self.labels_per_class = -1# help=\"number of labeled examples per class, if zero then use all labels\")\n",
    "        self.batch_size = 64\n",
    "        self.n_epochs = JEMUtils.get_parameter(\"epochs\", 10)\n",
    "        # regularization\n",
    "        self.dropout_rate = 0.0\n",
    "        self.sigma = 3e-2 # help=\"stddev of gaussian noise to add to input, .03 works but .1 is more stable\")\n",
    "        self.weight_decay = 0.0\n",
    "        # network\n",
    "        self.norm = None # choices=[None, \"norm\", \"batch\", \"instance\", \"layer\", \"act\"], help=\"norm to add to weights, none works fine\")\n",
    "        # logging + evaluation\n",
    "        self.save_dir = './experiment'\n",
    "        self.ckpt_every = 10 # help=\"Epochs between checkpoint save\")\n",
    "        self.eval_every = 1 # help=\"Epochs between evaluation\")\n",
    "        self.print_every = 100 # help=\"Iterations between print\")\n",
    "        self.load_path = None # path for checkpoint to load\n",
    "        self.print_to_log = False #\", action=\"store_true\", help=\"If true, directs std-out to log file\")\n",
    "        self.n_valid = 5000 # number of validation images\n",
    "        \n",
    "        # set from inline dict\n",
    "        for key in param_dict:\n",
    "            #print(key, '->', param_dict[key])\n",
    "            setattr(self, key, param_dict[key])\n",
    "            \n",
    "    def run(self):\n",
    "        self.result = self.experiment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2079d959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base:\n",
    "    def compute(self, inp):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "740c25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random subset of data\n",
    "class DataSubset(Dataset):\n",
    "    def __init__(self, base_dataset, inds=None, size=-1):\n",
    "        self.base_dataset = base_dataset\n",
    "        if inds is None:\n",
    "            inds = np.random.choice(list(range(len(base_dataset))), size, replace=False)\n",
    "        self.inds = inds\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        base_ind = self.inds[index]\n",
    "        return self.base_dataset[base_ind]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "492460ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup Wide_ResNet\n",
    "# Uses The Google Research Authors, file wideresnet.py\n",
    "class F(nn.Module):\n",
    "    def __init__(self, depth=28, width=2, norm=None, dropout_rate=0.0, n_classes=10):\n",
    "        super(F, self).__init__()\n",
    "        self.f = wideresnet.Wide_ResNet(depth, width, norm=norm, dropout_rate=dropout_rate)\n",
    "        self.energy_output = nn.Linear(self.f.last_dim, 1)\n",
    "        self.class_output = nn.Linear(self.f.last_dim, n_classes)\n",
    "\n",
    "    def classify(self, x):\n",
    "        penult_z = self.f(x)\n",
    "        return self.class_output(penult_z).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1f637b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JEMUtils:\n",
    "    \n",
    "    #global get_data\n",
    "    \n",
    "    # various utilities\n",
    "    @staticmethod\n",
    "    def cycle(loader):\n",
    "        while True:\n",
    "            for data in loader:\n",
    "                yield data\n",
    "                \n",
    "                \n",
    "    # calculate loss and accuracy for periodic printout\n",
    "    \n",
    "    @staticmethod\n",
    "    def eval_classification(f, dload, device):\n",
    "        corrects, losses = [], []\n",
    "        for x_p_d, y_p_d in dload:\n",
    "            x_p_d, y_p_d = x_p_d.to(device), y_p_d.to(device)\n",
    "            logits = f.classify(x_p_d)\n",
    "            loss = nn.CrossEntropyLoss(reduce=False)(logits, y_p_d).cpu().numpy()\n",
    "            losses.extend(loss)\n",
    "            correct = (logits.max(1)[1] == y_p_d).float().cpu().numpy()\n",
    "            corrects.extend(correct)\n",
    "        loss = np.mean(losses)\n",
    "        correct = np.mean(corrects)\n",
    "        return correct, loss\n",
    "    \n",
    "    \n",
    "    # save checkpoint data\n",
    "    \n",
    "    @staticmethod\n",
    "    def checkpoint(f, opt, epoch_no, tag, args, device):\n",
    "        f.cpu()\n",
    "        ckpt_dict = {\n",
    "            \"model_state_dict\": f.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "            'epoch': epoch_no,\n",
    "            #\"replay_buffer\": buffer\n",
    "        }\n",
    "        t.save(ckpt_dict, os.path.join(os.path.join(args.save_dir, args.experiment), tag))\n",
    "        f.to(device)\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def get_parameter(name, default):\n",
    "        params = yaml.safe_load(open(\"params.yaml\"))\n",
    "        to_search = params\n",
    "        for part in name.split(\".\"):\n",
    "            result = to_search.get(part)\n",
    "            if result == None:\n",
    "                return default\n",
    "            to_search = result\n",
    "        return to_search\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_data(args):\n",
    "        im_sz = 32\n",
    "        \n",
    "        #def lambdaForTransform(x):\n",
    "        #    return x + args.sigma * t.randn_like(x)\n",
    "        \n",
    "        #global transform_train\n",
    "        transform_train = tr.Compose(\n",
    "            [tr.Pad(4, padding_mode=\"reflect\"),\n",
    "             tr.RandomCrop(im_sz),\n",
    "             tr.RandomHorizontalFlip(),\n",
    "             tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             #tr.Lambda(lambdaForTransform)\n",
    "            #]\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "        #global transform_test\n",
    "        transform_test = tr.Compose(\n",
    "            [tr.ToTensor(),\n",
    "             tr.Normalize((.5, .5, .5), (.5, .5, .5)),\n",
    "             #tr.Lambda(lambdaForTransform)\n",
    "            #]\n",
    "             lambda x: x + args.sigma * t.randn_like(x)]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        def dataset_fn(train, transform):\n",
    "            return tv.datasets.CIFAR10(root='./dataset', transform=transform, download=True, train=train)\n",
    "\n",
    "        # get all training inds\n",
    "        #global full_train\n",
    "        full_train = dataset_fn(train=True, transform=transform_train)\n",
    "        all_inds = list(range(len(full_train)))\n",
    "        # set seed\n",
    "        np.random.seed(args.seed)\n",
    "        # shuffle\n",
    "        np.random.shuffle(all_inds)\n",
    "        # seperate out validation set\n",
    "        if args.n_valid is not None:\n",
    "            valid_inds, train_inds = all_inds[:args.n_valid], all_inds[args.n_valid:]\n",
    "        else:\n",
    "            valid_inds, train_inds = [], all_inds\n",
    "        train_inds = np.array(train_inds)\n",
    "        train_labeled_inds = []\n",
    "        other_inds = []\n",
    "        train_labels = np.array([full_train[ind][1] for ind in train_inds])\n",
    "        if args.labels_per_class > 0:\n",
    "            for i in range(args.n_classes):\n",
    "                print(i)\n",
    "                train_labeled_inds.extend(train_inds[train_labels == i][:args.labels_per_class])\n",
    "                other_inds.extend(train_inds[train_labels == i][args.labels_per_class:])\n",
    "        else:\n",
    "            train_labeled_inds = train_inds\n",
    "\n",
    "        dset_train = DataSubset(\n",
    "            dataset_fn(train=True, transform=transform_train),\n",
    "            inds=train_inds)\n",
    "        dset_train_labeled = DataSubset(\n",
    "            dataset_fn(train=True, transform=transform_train),\n",
    "            inds=train_labeled_inds)\n",
    "        dset_valid = DataSubset(\n",
    "            dataset_fn(train=True, transform=transform_test),\n",
    "            inds=valid_inds)\n",
    "        dload_train = DataLoader(dset_train, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "        dload_train_labeled = DataLoader(dset_train_labeled, batch_size=args.batch_size, shuffle=True, num_workers=0, drop_last=True)\n",
    "        dload_train_labeled = JEMUtils.cycle(dload_train_labeled)\n",
    "        dset_test = dataset_fn(train=False, transform=transform_test)\n",
    "        dload_valid = DataLoader(dset_valid, batch_size=100, shuffle=False, num_workers=0, drop_last=False)\n",
    "        dload_test = DataLoader(dset_test, batch_size=100, shuffle=False, num_workers=0, drop_last=False)\n",
    "        return dload_train, dload_train_labeled, dload_valid, dload_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "12323c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer(Base):\n",
    "    \n",
    "    def compute(self, inp):\n",
    "        args = inp\n",
    "\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        \n",
    "        if not os.path.exists(os.path.join(args.save_dir, args.experiment)):\n",
    "            os.makedirs(os.path.join(args.save_dir, args.experiment))\n",
    "\n",
    "        if args.print_to_log:\n",
    "            sys.stdout = open(f'{os.path.join(args.save_dir, args.experiment)}/log.txt', 'w')\n",
    "\n",
    "        t.manual_seed(args.seed)\n",
    "        if t.cuda.is_available():\n",
    "            t.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "        # datasets\n",
    "        \n",
    "        dload_train, dload_train_labeled, dload_valid, dload_test = JEMUtils.get_data(args)\n",
    "\n",
    "        device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "        # setup Wide_ResNet\n",
    "        f = F(args.depth, args.width, args.norm, dropout_rate=args.dropout_rate, n_classes=args.n_classes)\n",
    "    \n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "\n",
    "        # optimizer\n",
    "        params = f.class_output.parameters() if args.clf_only else f.parameters()\n",
    "        optim = t.optim.Adam(params, lr=args.lr, betas=[.9, .999], weight_decay=args.weight_decay)\n",
    "\n",
    "        # epoch_start\n",
    "        epoch_start = 0\n",
    "    \n",
    "        # load checkpoint?\n",
    "        if args.load_path:\n",
    "            print(f\"loading model from {os.path.join(args.load_path, args.experiment)}\")\n",
    "            ckpt_dict = t.load(os.path.join(args.load_path, args.experiment))\n",
    "            f.load_state_dict(ckpt_dict[\"model_state_dict\"])\n",
    "            optim.load_state_dict(ckpt_dict['optimizer_state_dict'])\n",
    "            epoch_start = ckpt_dict['epoch']\n",
    "\n",
    "        # push to GPU\n",
    "        f = f.to(device)\n",
    "    \n",
    "        # Show train set loss/accuracy after reload\n",
    "        f.eval()\n",
    "        with t.no_grad():\n",
    "            correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "            print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch_start, loss, correct))\n",
    "        f.train()\n",
    "\n",
    "        best_valid_acc = 0.0\n",
    "        cur_iter = 0\n",
    "    \n",
    "        # loop over epochs\n",
    "        scores = {}\n",
    "        for epoch in range(epoch_start, epoch_start + args.n_epochs):\n",
    "            # loop over data in batches\n",
    "            # x_p_d sample from dataset\n",
    "            for i, (x_p_d, _) in enumerate(dload_train): #tqdm(enumerate(dload_train)):\n",
    "\n",
    "                #print(\"x_p_d_shape\",x_p_d.shape)\n",
    "                x_p_d = x_p_d.to(device)\n",
    "                x_lab, y_lab = dload_train_labeled.__next__()\n",
    "                x_lab, y_lab = x_lab.to(device), y_lab.to(device)\n",
    "\n",
    "                # initialize loss\n",
    "                L = 0.\n",
    "            \n",
    "                # normal cross entropy loss function\n",
    "                # maximize log p(y | x)\n",
    "                logits = f.classify(x_lab)\n",
    "                l_p_y_given_x = nn.CrossEntropyLoss()(logits, y_lab)\n",
    "                if cur_iter % args.print_every == 0:\n",
    "                    acc = (logits.max(1)[1] == y_lab).float().mean()\n",
    "                    print('P(y|x) {}:{:>d} loss={:>14.9f}, acc={:>14.9f}'.format(epoch,\n",
    "                                                                             cur_iter,\n",
    "                                                                             l_p_y_given_x.item(),\n",
    "                                                                             acc.item()))\n",
    "                # add to loss\n",
    "                L += l_p_y_given_x\n",
    "\n",
    "                # break if the loss diverged\n",
    "                if L.abs().item() > 1e8:\n",
    "                    print(\"Divergwence error\")\n",
    "                    1/0\n",
    "\n",
    "                # Optimize network using our loss function L\n",
    "                optim.zero_grad()\n",
    "                L.backward()\n",
    "                optim.step()\n",
    "                cur_iter += 1\n",
    "\n",
    "            # do checkpointing\n",
    "            if epoch % args.ckpt_every == 0:\n",
    "                JEMUtils.checkpoint(f, optim, epoch, f'ckpt_{epoch}.pt', args, device)\n",
    "\n",
    "            # Print performance assesment \n",
    "            if epoch % args.eval_every == 0:\n",
    "                f.eval()\n",
    "                with t.no_grad():\n",
    "                    # train set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_train, device)\n",
    "                    scores[\"train\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Train Loss {}, Train Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # test set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_test, device)\n",
    "                    scores[\"test\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Test Loss {}, Test Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                    # validation set\n",
    "                    correct, loss = JEMUtils.eval_classification(f, dload_valid, device)\n",
    "                    scores[\"validation\"] = {\"acc:\": float(correct), \"loss\": float(loss)}\n",
    "                    print(\"Epoch {}: Valid Loss {}, Valid Acc {}\".format(epoch, loss, correct))\n",
    "\n",
    "                f.train()\n",
    "\n",
    "            # do \"last\" checkpoint\n",
    "            JEMUtils.checkpoint(f, optim, epoch, \"last_ckpt.pt\", args, device)\n",
    "\n",
    "        # write stats\n",
    "        with open(os.path.join(args.save_dir, args.experiment) + '_scores.json', 'w') as outfile:\n",
    "            json.dump(scores, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "57eff4b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 15:20:25,086 (WARNING): Jupyter support is an experimental feature! Please save your notebook before running this command!\n",
      "Submit issues to https://github.com/zincware/ZnTrack.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ZnTrain.ipynb to script\n",
      "[NbConvertApp] Writing 15204 bytes to ZnTrain.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@Node()\n",
    "class XEntropyAugmented:\n",
    "    \n",
    "    args: train_args = dvc.deps(train_args(load=True))\n",
    "    trainer: Base = zn.Method()\n",
    "    result = zn.metrics()\n",
    "    \n",
    "            \n",
    "    def __call__(self, operation):\n",
    "        self.trainer = operation\n",
    "    \n",
    "    def run(self):\n",
    "        self.result = self.args.result\n",
    "        self.result += self.trainer.compute(self.args)\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "98380c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 13:35:40,608 (WARNING): --- Writing new DVC file! ---\n",
      "2021-12-21 13:35:41,411 (INFO): Modifying stage 'train_args' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inline_parms = {\"lr\": .0001, \"experiment\": 'x-entropy_augmented'} #, \"load_path\": './all1/last_ckpt.pt'}\n",
    "\n",
    "params = train_args()\n",
    "params(param_dict=inline_parms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "05cb0ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-21 15:20:59,309 (ERROR): Can not convert args!\n",
      "2021-12-21 15:20:59,310 (ERROR): Can not convert kwargs!\n",
      "2021-12-21 15:20:59,350 (WARNING): --- Writing new DVC file! ---\n",
      "2021-12-21 15:21:00,202 (INFO): Modifying stage 'XEntropyAugmented' in 'dvc.yaml'\n",
      "\n",
      "To track the changes with git, run:\n",
      "\n",
      "    git add dvc.yaml\n",
      "\n",
      "To enable auto staging, run:\n",
      "\n",
      "\tdvc config core.autostage true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "\n",
    "runner = XEntropyAugmented()\n",
    "runner(operation=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe17e4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
